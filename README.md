# info-retrieval
#### _Made by Almaz Khamedzhanov and Guzel Musina 11-702_

### _Из чего состоит проект_
1. Файл `index.html`, создан по структуре тега `<document>`, у которого есть уникальный идентификатор  `id` и тег `<url>`, в котором располагается url страницы. 
Данный файл сделан для удобства считывания url и передачи его для краулера.
2. Файл `index.txt`, структура `id url` на каждой строке.
3. Папка pages_html, в которой хранятся 100 html страниц
4. Файл `Parser.py`, в котором описана логика краулера (подробнее про работу класса, можно прочитать в комментариях, которые написаны к классу)

### _How to start_
1. Скачать проект к себе на компьютер
2. Установить пакет `pip install scrapy`
3. Для чистоты эксперимента, можно удалить файлы, которые находятся в папке `pages_html`
4. Запустить код программой `scrapy runspider Parser.py`, либо через `Run` в среде исполнения (например в PyCharm, Anaconda)