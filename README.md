# info-retrieval
#### _Made by Almaz Khamedzhanov and Guzel Musina 11-702_

### _Из чего состоит проект_
1. Файл `index.html`, создан по структуре тега `<document>`, у которого есть уникальный идентификатор  `id` и тег `<url>`, в котором располагается url страницы. 
Данный файл сделан для удобства считывания url и передачи его для краулера.
2. Файл `index.txt`, структура `page_id url` на каждой строке.
3. Папка `pages_html`, в которой хранятся 100 html страниц. Каждая страница имеет название page_id.html. 
4. Файл `Parser.py`, в котором описана логика краулера (подробнее про работу класса, можно прочитать в комментариях, которые написаны к классу)
5. Файл `Normalyze.py`, в котором отделяются слова из 100 html файлов в `pages.html`, записываются в файл `word_list.txt`. Далее происходит лемматизация токенов и запись их в `output.txt`.
6. Файл  `word_list.txt` -  файл по списком слов (Формат строк: <слово><\n>).
7. Файл `output.txt` файл по списком лемматизированных токенов.

### _How to start_ ДЗ 1
1. Скачать проект к себе на компьютер
2. Установить пакет `pip install scrapy`
3. Для чистоты эксперимента, можно удалить файлы, которые находятся в папке `pages_html`
4. Запустить код программой `scrapy runspider Parser.py`, либо через `Run` в среде исполнения (например в PyCharm, Anaconda)

### _How to start_ ДЗ 2
1. Если сделано все из ДЗ 1, то необходимо установить `pip install codecs`.
2. Установить `pip install pymorphy2`.
3. Установить `pip install bs4`.
4. Установить `pip install nltk`.
5. Для чистоты эксперимента, можно удалить файлы  `word_list.txt` и  `output.txt`.
6. Запустить файл `Normalize.py` программой `python Normalize.py`, либо через `Run` в среде исполнения (например в PyCharm, Anaconda).
